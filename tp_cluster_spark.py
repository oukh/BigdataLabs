# -*- coding: utf-8 -*-
"""TP: Cluster spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11egJ07CUxh4dC45kkQY_4smNMkz6wSzP
"""

!sudo apt update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# Télécharger Apache Spark (vérifier la dernière version si besoin)
!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!tar xf spark-3.2.1-bin-hadoop3.2.tgz

# Installer les packages Python
!pip install -q findspark
!pip install pyspark
!pip install py4j
!pip install -q pymongo matplotlib seaborn

import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder \
.appName("ColabSpark") \
.config("spark.driver.memory", "2g") \
.getOrCreate()
print(" Spark est configuré avec succès !")

print(spark.version)

data = [(1, "Alice", 23), (2, "Bob", 30), (3, "Charlie", 29)]
columns = ["id", "nom", "age"]

df = spark.createDataFrame(data, columns)

df.show()

df.printSchema()                 # Affiche la structure
df.select("nom", "age").show()   # Sélectionne certaines colonnes
df.filter(df.age > 25).show()    # Filtre les lignes où age > 25

df.printSchema() # Structure du DataFrame
df.select("nom", "age").show() # Sélection de colonnes
df.filter(df.age > 25).show() # Filtrage des données

df = spark.read.csv("/content/transaction_data.csv", header=True, inferSchema=True)
df.show(5)

df.printSchema()

df.filter(df["Transaction Amount"] > 1000).show()

df.groupBy("Transaction Type").sum("Transaction Amount").show()

df.orderBy(df["Transaction Amount"].desc()).show(5)